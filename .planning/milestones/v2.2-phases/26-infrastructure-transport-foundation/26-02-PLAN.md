---
phase: 26-infrastructure-transport-foundation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src-tauri/Cargo.toml
  - src-tauri/src/telemetry.rs
  - src-tauri/src/lib.rs
  - src-tauri/tauri.conf.json
  - .github/workflows/release.yml
  - .env.example
autonomous: true
requirements:
  - TELE-04
  - TELE-08

must_haves:
  truths:
    - "The Rust `ph_send_batch` command is registered in the Tauri binary and accepts a JSON event batch"
    - "PostHog endpoints are present in both `csp` and `devCsp` connect-src"
    - "PostHog API key is read from VITE_POSTHOG_KEY env var, never hardcoded in source"
    - "Events are queued in a separate telemetry.db when network is unavailable"
    - "The offline queue persists across app restarts"
  artifacts:
    - path: "src-tauri/src/telemetry.rs"
      provides: "ph_send_batch command, TelemetryState, offline queue logic"
      exports: ["ph_send_batch", "TelemetryState"]
    - path: "src-tauri/src/lib.rs"
      provides: "Registration of ph_send_batch in invoke_handler and TelemetryState in managed state"
      contains: "ph_send_batch"
    - path: "src-tauri/Cargo.toml"
      provides: "reqwest and sqlx direct dependencies"
      contains: "reqwest"
    - path: "src-tauri/tauri.conf.json"
      provides: "PostHog CSP entries in both csp and devCsp"
      contains: "eu.i.posthog.com"
  key_links:
    - from: "src-tauri/src/lib.rs"
      to: "src-tauri/src/telemetry.rs"
      via: "mod telemetry + invoke_handler registration"
      pattern: "mod telemetry"
    - from: "src-tauri/src/telemetry.rs"
      to: "reqwest"
      via: "HTTP POST to PostHog batch endpoint"
      pattern: "reqwest::Client"
    - from: "src-tauri/src/telemetry.rs"
      to: "sqlx"
      via: "SQLite pool for offline event queue"
      pattern: "sqlx::SqlitePool"

user_setup:
  - service: PostHog
    why: "Telemetry event ingestion (Phase 27 will use it; Phase 26 just builds the transport)"
    env_vars:
      - name: VITE_POSTHOG_KEY
        source: "PostHog Dashboard -> Project Settings -> API Keys -> Project API Key"
    dashboard_config:
      - task: "Create PostHog project (EU region for GDPR)"
        location: "https://eu.posthog.com -> New Project"
---

<objective>
Build the Rust IPC relay command (`ph_send_batch`) with SQLite-backed offline queue, update CSP for PostHog endpoints, and configure the PostHog API key as an environment variable.

Purpose: Phase 27 telemetry will invoke `ph_send_batch` from the frontend instead of using `posthog-js` directly (which silently drops events in Tauri WebView). The relay ensures reliable delivery with automatic offline buffering and queue flushing.
Output: Rust `telemetry.rs` module, updated `lib.rs` with command registration, CSP entries for PostHog, env var setup.
</objective>

<execution_context>
@C:/Users/Boris/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Boris/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/26-infrastructure-transport-foundation/26-RESEARCH.md

@src-tauri/Cargo.toml
@src-tauri/src/lib.rs
@src-tauri/tauri.conf.json
@.github/workflows/release.yml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Rust telemetry module with ph_send_batch and offline queue</name>
  <files>src-tauri/Cargo.toml, src-tauri/src/telemetry.rs, src-tauri/src/lib.rs</files>
  <action>
1. **Update `src-tauri/Cargo.toml`** — add direct dependencies (already in Cargo.lock as transitive deps):
   ```toml
   [dependencies]
   # ... existing deps ...
   reqwest = { version = "0.12", features = ["json", "rustls-tls"], default-features = false }
   sqlx = { version = "0.8", features = ["runtime-tokio", "sqlite"] }
   ```

2. **Create `src-tauri/src/telemetry.rs`** with the following structure:

   **Types:**
   - `PhEvent` (serde Deserialize+Serialize): `{ event: String, properties: serde_json::Value, timestamp: Option<String> }`
   - `BatchPayload` (serde Deserialize): `{ events: Vec<PhEvent>, api_key: String }`
   - `BatchResult` (serde Serialize): `{ sent: usize, queued: usize }`
   - `TelemetryState`: `{ pool: SqlitePool, api_host: String }` — managed by Tauri

   **Constants:**
   - `QUEUE_SCHEMA`: SQL string to create `ph_event_queue` table and index
   - `MAX_QUEUE_SIZE`: 500 (prune oldest beyond this)
   - `MAX_RETRY_COUNT`: 5 (discard events that failed 5+ times)
   - `HTTP_TIMEOUT`: 10 seconds

   **Queue schema** (separate `telemetry.db`):
   ```sql
   CREATE TABLE IF NOT EXISTS ph_event_queue (
       id INTEGER PRIMARY KEY AUTOINCREMENT,
       event_json TEXT NOT NULL,
       created_at INTEGER NOT NULL,
       retry_count INTEGER NOT NULL DEFAULT 0
   );
   CREATE INDEX IF NOT EXISTS idx_queue_created ON ph_event_queue(created_at ASC);
   ```

   **Command `ph_send_batch`** (async, returns `Result<BatchResult, String>`):
   - Accept `payload: BatchPayload` and `state: State<'_, TelemetryState>`
   - Create `reqwest::Client::new()` (consider caching in TelemetryState if perf matters — Claude's discretion)
   - Build JSON body: `{ "api_key": payload.api_key, "batch": payload.events }`
   - POST to `{state.api_host}/batch` with 10-second timeout
   - On success (2xx): also call `flush_queue()` to send any queued events, return `{ sent: events.len(), queued: 0 }`
   - On failure (network error, non-2xx): call `queue_events()` to persist events, return `{ sent: 0, queued: count }`

   **Helper `queue_events`** (async):
   - INSERT each event as JSON into `ph_event_queue` with `created_at = current Unix timestamp in ms`
   - After insert, enforce `MAX_QUEUE_SIZE` by deleting oldest rows beyond limit
   - Return count of inserted events

   **Helper `flush_queue`** (async):
   - SELECT up to 50 events from `ph_event_queue` ordered by `created_at ASC` where `retry_count < MAX_RETRY_COUNT`
   - If no events, return immediately
   - Build batch body and POST to PostHog
   - On success: DELETE the sent rows from queue
   - On failure: INCREMENT `retry_count` for those rows
   - DELETE any rows where `retry_count >= MAX_RETRY_COUNT` (give up after 5 retries)

   **Public function `init_telemetry_db`** (async, called from lib.rs setup):
   - Accept `app_data_dir: &std::path::Path`
   - `std::fs::create_dir_all(app_data_dir)` to ensure directory exists
   - Open `telemetry.db` in `app_data_dir` via `sqlx::sqlite::SqlitePoolOptions::new().max_connections(1).connect(...)`
   - Run `QUEUE_SCHEMA` to create table if not exists
   - Return `SqlitePool`

3. **Update `src-tauri/src/lib.rs`**:
   - Add `mod telemetry;` at the top
   - Add `use telemetry::ph_send_batch;` import
   - In `.invoke_handler()`, change: `tauri::generate_handler![force_quit]` to `tauri::generate_handler![force_quit, telemetry::ph_send_batch]`
   - In `.setup(|app| { ... })`, BEFORE the tray icon setup:
     ```rust
     // Initialize telemetry DB (separate from main app DB)
     let data_dir = app.path().app_data_dir()
         .expect("app data dir unavailable");
     let telemetry_pool = tauri::async_runtime::block_on(
         telemetry::init_telemetry_db(&data_dir)
     );
     app.manage(telemetry::TelemetryState {
         pool: telemetry_pool,
         api_host: "https://eu.i.posthog.com".to_string(),
     });
     ```
   - Also flush the queue at startup: `tauri::async_runtime::block_on(telemetry::startup_flush(app.state::<telemetry::TelemetryState>()));` — This attempts to send any events that were queued before last shutdown.

**Key constraints (locked decisions):**
- Use a **separate `telemetry.db`**, NOT the main app database — avoids schema coupling with `tauri-plugin-sql`
- Queue must **persist across app restarts** — SQLite file is never deleted
- Events are **buffered when network is unavailable**, flushed when connectivity returns (successful send triggers flush)
- **No event loss** on crash/close — SQLite WAL mode handles this

**Anti-patterns to avoid:**
- Do NOT use `tauri_plugin_sql` Rust API — it has NO Rust-side API
- Do NOT share SqlitePool with the main app DB
- Do NOT forget to add `ph_send_batch` to `generate_handler![]`
- Do NOT forget `default-features = false` for reqwest (avoids pulling in native-tls)
  </action>
  <verify>
- `pnpm tauri build` compiles the Rust binary without errors
- `ph_send_batch` appears in the `generate_handler![]` macro
- `telemetry.rs` compiles with proper types
- The telemetry DB initialization runs in the setup closure
  </verify>
  <done>Rust `ph_send_batch` command registered in Tauri binary; SQLite offline queue with separate `telemetry.db`; queue persists across restarts; flush on startup and after successful sends; `pnpm tauri build` succeeds.</done>
</task>

<task type="auto">
  <name>Task 2: Update CSP for PostHog endpoints and configure env var</name>
  <files>src-tauri/tauri.conf.json, .github/workflows/release.yml, .env.example</files>
  <action>
1. **Update `src-tauri/tauri.conf.json`** — add PostHog endpoints to `connect-src` in both `csp` and `devCsp`:

   In `app.security.csp.connect-src`, add `https://eu.i.posthog.com https://us.i.posthog.com` BEFORE the trailing `https:` wildcard:
   ```
   "connect-src": "ipc: http://ipc.localhost http://localhost:* http://127.0.0.1:* https://api.groq.com https://generativelanguage.googleapis.com https://api.openai.com https://eu.i.posthog.com https://us.i.posthog.com https:"
   ```

   In `app.security.devCsp.connect-src`, same addition:
   ```
   "connect-src": "ipc: http://ipc.localhost http://localhost:* http://127.0.0.1:* ws://localhost:* https://api.groq.com https://generativelanguage.googleapis.com https://api.openai.com https://eu.i.posthog.com https://us.i.posthog.com https:"
   ```

   Note: The `https:` wildcard already allows all HTTPS traffic, but explicit entries document the intentional PostHog integration for code reviewers.

2. **Update `.github/workflows/release.yml`** — add `VITE_POSTHOG_KEY` env var to the "Build and release" step:
   ```yaml
   - name: Build and release
     uses: tauri-apps/tauri-action@v0.6.1
     env:
       GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
       TAURI_SIGNING_PRIVATE_KEY: ${{ secrets.TAURI_SIGNING_PRIVATE_KEY }}
       TAURI_SIGNING_PRIVATE_KEY_PASSWORD: ${{ secrets.TAURI_SIGNING_PRIVATE_KEY_PASSWORD }}
       VITE_POSTHOG_KEY: ${{ secrets.VITE_POSTHOG_KEY }}
   ```

3. **Create `.env.example`** (if it does not exist) or update it — document the PostHog key:
   ```
   # PostHog telemetry (optional — telemetry disabled if not set)
   VITE_POSTHOG_KEY=
   ```
   This file is committed to git as documentation. The actual `.env.local` with real values stays gitignored.

4. Verify `.env.local` is in `.gitignore` (it should already be — Vite convention).

**Anti-pattern to avoid:**
- Do NOT put a default/fallback PostHog API key in source code — if env var is missing, telemetry simply does not activate (graceful degradation).
  </action>
  <verify>
- `src-tauri/tauri.conf.json` contains `eu.i.posthog.com` and `us.i.posthog.com` in BOTH `csp` and `devCsp` `connect-src`
- `.github/workflows/release.yml` has `VITE_POSTHOG_KEY: ${{ secrets.VITE_POSTHOG_KEY }}` in the build step env
- `.env.example` exists with `VITE_POSTHOG_KEY=` entry
- `pnpm build` still passes (CSP is only used at runtime, not build time)
  </verify>
  <done>PostHog endpoints in CSP (both prod and dev); VITE_POSTHOG_KEY configured in release workflow via GitHub Secrets; .env.example documents the env var; no API key hardcoded in source.</done>
</task>

</tasks>

<verification>
1. `pnpm tauri build` compiles successfully with `ph_send_batch` registered
2. `grep -c "eu.i.posthog.com" src-tauri/tauri.conf.json` returns 2 (one in csp, one in devCsp)
3. `grep -c "us.i.posthog.com" src-tauri/tauri.conf.json` returns 2
4. `grep "VITE_POSTHOG_KEY" .github/workflows/release.yml` shows the env var
5. `telemetry.rs` contains `ph_send_batch`, `queue_events`, `flush_queue`, `init_telemetry_db`
6. `lib.rs` contains `mod telemetry` and `ph_send_batch` in generate_handler
7. `Cargo.toml` contains `reqwest` and `sqlx` as direct dependencies
</verification>

<success_criteria>
- Rust `ph_send_batch` command registered in Tauri binary, accepts JSON event batch
- Offline queue uses separate `telemetry.db` with SQLite, persists across app restarts
- Queue flushes automatically after successful sends and on app startup
- Max queue size of 500 events enforced; events discarded after 5 failed retries
- PostHog endpoints in both `csp` and `devCsp` of `tauri.conf.json`
- VITE_POSTHOG_KEY env var in release workflow, never in source code
- `pnpm tauri build` succeeds
</success_criteria>

<output>
After completion, create `.planning/phases/26-infrastructure-transport-foundation/26-02-SUMMARY.md`
</output>
