---
phase: 22-provider-registry-core-refactor
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/types/aiProvider.ts
  - src/lib/ai-provider-registry.ts
  - src-tauri/tauri.conf.json
autonomous: true

must_haves:
  truths:
    - "Provider registry returns all 3 built-in providers (Groq, Gemini, OpenAI) with correct metadata"
    - "Custom providers can be loaded from and saved to localStorage with Zod validation"
    - "Custom provider endpoint validation enforces HTTPS or localhost"
    - "CSP connect-src allows custom HTTPS endpoints without wildcard"
    - "getProviderById returns correct config for both built-in and custom providers"
  artifacts:
    - path: "src/types/aiProvider.ts"
      provides: "Provider registry Zod schemas and TypeScript types"
      contains: "ProviderConfigSchema"
    - path: "src/lib/ai-provider-registry.ts"
      provides: "Centralized provider registry (SSOT)"
      exports: ["BUILT_IN_PROVIDERS", "getAllProviders", "getProviderById", "loadCustomProviders", "saveCustomProviders", "addCustomProvider", "removeCustomProvider", "validateCustomProvider"]
    - path: "src-tauri/tauri.conf.json"
      provides: "Updated CSP with https: for custom endpoints"
      contains: "https:"
  key_links:
    - from: "src/lib/ai-provider-registry.ts"
      to: "src/types/aiProvider.ts"
      via: "import ProviderConfigSchema"
      pattern: "import.*ProviderConfigSchema.*from.*aiProvider"
    - from: "src/lib/ai-provider-registry.ts"
      to: "localStorage"
      via: "loadCustomProviders/saveCustomProviders"
      pattern: "localStorage\\.(get|set)Item.*custom-ai-providers"
---

<objective>
Create the provider registry foundation: Zod-validated types, built-in provider definitions, custom provider CRUD with localStorage persistence, and CSP update for custom HTTPS endpoints.

Purpose: Establish the single source of truth for all AI provider configurations, replacing scattered hardcoded logic. This foundation enables Plans 02 and 03 to consume the registry.

Output: `src/types/aiProvider.ts`, `src/lib/ai-provider-registry.ts`, updated `src-tauri/tauri.conf.json`
</objective>

<execution_context>
@C:/Users/Boris/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Boris/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/22-provider-registry-core-refactor/22-RESEARCH.md
@src/types/projectAIConfig.ts
@src/constants/storage.ts
@src-tauri/tauri.conf.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create provider registry types with Zod schemas</name>
  <files>src/types/aiProvider.ts</files>
  <action>
Create `src/types/aiProvider.ts` with the following Zod schemas and types:

1. **ProviderType enum**: `z.enum(['groq', 'gemini', 'openai-compatible'])` — the SDK type used under the hood.

2. **ProviderConfigSchema**: Zod object with fields:
   - `id`: `z.string().regex(/^[a-z0-9-]+$/)` — unique identifier (e.g. "groq", "ollama-local")
   - `name`: `z.string().min(1).max(50)` — display name
   - `type`: ProviderType enum — which SDK to use
   - `baseURL`: `z.string().url().optional()` — for openai-compatible only
   - `defaultModel`: `z.string().min(1)` — default model ID
   - `models`: array of `{ id: string, name: string }` — available models
   - `capabilities`: object with `structuredOutput: boolean`, `multimodal: boolean`
   - `isCustom`: `z.boolean().default(false)` — distinguishes built-in from user-created

3. **CustomProviderInputSchema**: Zod object for user input when adding a custom provider:
   - `name`: `z.string().min(1).max(50)`
   - `baseURL`: `z.string().url()` with `.refine()` that enforces `url.startsWith('https://') || url.startsWith('http://localhost') || url.startsWith('http://127.0.0.1')` — message: "Custom endpoints must use HTTPS or localhost"
   - `apiKey`: `z.string().optional()` — optional for localhost providers like Ollama
   - `defaultModel`: `z.string().min(1)` — the model ID to use
   - `models`: `z.array(z.object({ id: z.string(), name: z.string() })).optional()` — user can optionally specify multiple models

4. Export all inferred types: `ProviderConfig`, `ProviderType`, `CustomProviderInput`

5. **BuiltInProviderId type**: `'groq' | 'gemini' | 'openai'` — literal union for the 3 built-in providers. This maintains backward compatibility with the existing `AIProvider` type.

Import `z` from `'zod'` (already in project dependencies, uses zod v4 with `import { z } from 'zod'`).
  </action>
  <verify>
`pnpm build` compiles without errors. The file exports ProviderConfigSchema, CustomProviderInputSchema, and all types.
  </verify>
  <done>
`src/types/aiProvider.ts` exists with Zod schemas for ProviderConfig and CustomProviderInput, BuiltInProviderId type alias, and all inferred types exported.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create provider registry module with built-in providers and custom provider CRUD</name>
  <files>src/lib/ai-provider-registry.ts</files>
  <action>
Create `src/lib/ai-provider-registry.ts` as the single source of truth for all provider configurations.

**Built-in providers array** (`BUILT_IN_PROVIDERS: ProviderConfig[]`):

1. **Groq**:
   - id: 'groq', name: 'Groq', type: 'groq'
   - defaultModel: 'llama-3.3-70b-versatile'
   - models: same as current `AVAILABLE_MODELS.groq` in `projectAIConfig.ts` (4 models: llama-3.3-70b-versatile, llama-3.1-70b-versatile, llama-3.1-8b-instant, mixtral-8x7b-32768)
   - capabilities: { structuredOutput: true, multimodal: false }
   - isCustom: false

2. **Gemini**:
   - id: 'gemini', name: 'Gemini', type: 'gemini'
   - defaultModel: 'gemini-2.0-flash'
   - models: same as current `AVAILABLE_MODELS.gemini` (3 models: gemini-2.0-flash, gemini-1.5-flash, gemini-1.5-pro)
   - capabilities: { structuredOutput: true, multimodal: true }
   - isCustom: false

3. **OpenAI**:
   - id: 'openai', name: 'OpenAI', type: 'openai-compatible'
   - baseURL: 'https://api.openai.com/v1'
   - defaultModel: 'gpt-4o-mini'
   - models: same as current `AVAILABLE_MODELS.openai` (4 models: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo)
   - capabilities: { structuredOutput: true, multimodal: true }
   - isCustom: false

**Storage constant**: `const CUSTOM_PROVIDERS_KEY = 'custom-ai-providers'`

**Exported functions**:

1. `loadCustomProviders(): ProviderConfig[]` — Read from localStorage, parse with `z.array(ProviderConfigSchema)`, return `[]` on failure. Wrap in try-catch.

2. `saveCustomProviders(providers: ProviderConfig[]): void` — Stringify and save to localStorage.

3. `getAllProviders(): ProviderConfig[]` — Return `[...BUILT_IN_PROVIDERS, ...loadCustomProviders()]`.

4. `getProviderById(id: string): ProviderConfig | null` — Find in `getAllProviders()`.

5. `getBuiltInProvider(id: BuiltInProviderId): ProviderConfig` — Direct lookup in `BUILT_IN_PROVIDERS` (guaranteed to exist). Throws if not found (programming error).

6. `addCustomProvider(input: CustomProviderInput): { success: true; provider: ProviderConfig } | { success: false; error: string }` — Validate with `CustomProviderInputSchema`, generate ID as `custom-${name.toLowerCase().replace(/\s+/g, '-').replace(/[^a-z0-9-]/g, '')}`, check for duplicate ID, build full ProviderConfig (type: 'openai-compatible', capabilities: { structuredOutput: false, multimodal: false } as conservative defaults), append to custom providers list, save.

7. `removeCustomProvider(id: string): boolean` — Remove from custom providers list. Return false if not found or if trying to remove a built-in. Save after removal.

8. `validateCustomProvider(input: unknown): { success: boolean; error?: string; data?: CustomProviderInput }` — Safe parse with CustomProviderInputSchema, return structured result.

9. `isBuiltInProvider(id: string): boolean` — Check if id matches any BUILT_IN_PROVIDERS entry.

10. `getDefaultModelForProvider(providerId: string): string | null` — Look up provider and return its defaultModel.

Import types from `../types/aiProvider`.
  </action>
  <verify>
`pnpm build` compiles without errors. The module exports all 10 functions and the BUILT_IN_PROVIDERS constant.
  </verify>
  <done>
`src/lib/ai-provider-registry.ts` exists as a complete provider registry with built-in provider definitions matching current codebase, custom provider CRUD with Zod validation, and localStorage persistence.
  </done>
</task>

<task type="auto">
  <name>Task 3: Update CSP to allow custom HTTPS endpoints</name>
  <files>src-tauri/tauri.conf.json</files>
  <action>
Update the CSP `connect-src` in `src-tauri/tauri.conf.json` to support custom HTTPS endpoints.

**Strategy: HTTPS-only enforcement (per research recommendation Option A)**

In the production `csp.connect-src`, add `https:` to the whitelist. This allows connections to ANY HTTPS endpoint while blocking HTTP (except localhost for dev). The full value becomes:

```
"connect-src": "ipc: http://ipc.localhost https://api.groq.com https://generativelanguage.googleapis.com https://api.openai.com https:"
```

Note: `https:` at the end is a CSP scheme-source that matches any HTTPS URL. The specific domain entries (api.groq.com, etc.) are kept for documentation clarity but are technically redundant with `https:`. They serve as explicit documentation of known providers.

In the `devCsp.connect-src`, also add `https:`:

```
"connect-src": "ipc: http://ipc.localhost http://localhost:* ws://localhost:* https://api.groq.com https://generativelanguage.googleapis.com https://api.openai.com https:"
```

**IMPORTANT**: Do NOT use wildcard `https://*`. Use the scheme-source `https:` which is the CSP standard way to allow all HTTPS connections. This is more secure than wildcard because it explicitly only matches HTTPS (no HTTP to arbitrary domains).

Also add `http://localhost:* http://127.0.0.1:*` to the production `connect-src` for local AI providers (Ollama, LM Studio):

```
"connect-src": "ipc: http://ipc.localhost http://localhost:* http://127.0.0.1:* https://api.groq.com https://generativelanguage.googleapis.com https://api.openai.com https:"
```

And add `http://127.0.0.1:*` to the devCsp if not already present:

```
"connect-src": "ipc: http://ipc.localhost http://localhost:* http://127.0.0.1:* ws://localhost:* https://api.groq.com https://generativelanguage.googleapis.com https://api.openai.com https:"
```
  </action>
  <verify>
`pnpm build` passes. Verify the updated CSP in `src-tauri/tauri.conf.json` contains `https:` and `http://localhost:*` in both `csp` and `devCsp` connect-src directives.
  </verify>
  <done>
CSP updated to allow HTTPS scheme-source and localhost for custom provider endpoints. Both production and dev CSP configurations are updated consistently.
  </done>
</task>

</tasks>

<verification>
1. `pnpm build` passes without errors
2. New files exist: `src/types/aiProvider.ts`, `src/lib/ai-provider-registry.ts`
3. `src-tauri/tauri.conf.json` has updated CSP with `https:` and `http://localhost:*`
4. No existing functionality is broken (no imports from existing files changed)
5. Registry returns 3 built-in providers with correct metadata
6. Custom provider validation correctly rejects HTTP non-localhost URLs
</verification>

<success_criteria>
- Provider registry module compiles and exports all documented functions
- Built-in providers match existing model lists in projectAIConfig.ts
- Custom provider CRUD operations work with localStorage
- CSP allows custom HTTPS endpoints and localhost connections
- Zero impact on existing functionality (this plan only adds new files + CSP update)
</success_criteria>

<output>
After completion, create `.planning/phases/22-provider-registry-core-refactor/22-01-SUMMARY.md`
</output>
